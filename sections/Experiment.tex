\section{Experiments}

\subsection{Implementation Details}
\label{sec:impl-deta}

We implemented three variants of algorithm (\ref{alg:dcsc}) for the cases $\ell = 1,2,3$. For the single layer case we let $\W^T_1$ be a 11x11 convolution with striding of 2 and 8 filters. For $\ell = 2$ we let $\W^T_{1}$ be an 11x11 convolution with 8 filters and $W^T_{1}$ be a 7x7 convolution with 16 filters. Finally for the $\ell = 3$ case: $\W^T_{1}$ is an 11x11 convolution with 8 filters, $\W^T_{2}$ is a 5x5 convolution with 16 filters, and $\W^T_{3}$ is a 3x3 convolution with 32 filters. For both $\ell = 2$ and $\ell = 3$, all convolutions have striding of 2. For the single layer case we learned the dictionaries with the number of iterations set to 5 and then at test time increased the number of iterations to 20. For the two and three layer cases the number of iterations was fixed at train and test time to 10 except in section \ref{sec:effect-iter-optim} where the number of test and training iterations is varied. All training was done with the ADAM optimizer with the standard parameters: learning rate = 0.001,$\beta_1 = 0.9$, $\beta_2$ =0.999, $\epsilon = 10^{-8}$.


\subsubsection{Error Metrics}
\label{sec:error-metrics}

For evaluation on the KITTI benchmark we use the conventional error metrics~\cite{uhrig,Geiger2012CVPR}, \eg root mean square error (RMSE), mean absolute error (MAE), mean absolute relative error (MRE). We also use the percentage of inliers metric, $\delta_i$ which counts the percent of predictions whose relative error is within a threshold raised to the power $i$. Here, we use smaller thresholds ($1.01^i$) compared to the more widely used ones ($1.5^i$) in oder to compare differences in performance under tighter metrics.
% \begin{itemize}
% \item RMSE: root mean square error
% \item MAE: mean absolute error
% \item MRE: mean absolute relative error
% \item percent of predictions whose relative error is within a threshold $\delta_i$. Formally:

 
% \end{itemize}
\subsection{KITTI Depth Completion Benchmark}
\label{sec:kitti-depth-compl}
% \begin{figure}
\begin{table}
\centering
\begin{tabular}{r|cccccc}
  & RMSE (m) & MAE (m) & MRE & $\delta_1<1.01$ & $\delta_2<1.01^2$ & $\delta_3<1.01^3$\\\hline
  Bilateral NN\cite{Jampani2016CVPR} & 4.19 & 1.09 & - & - & - & -\\
  SGDU\cite{schneider2016semantically} & 2.5 & 0.72 & - & - & - & -\\
  Fast Bilateral Solver\cite{barron2016fast} & 1.98 & 0.65 & - & - & - & -\\
  TGVL\cite{Ferstl2013} & 4.85 & 0.59 & - & - & - & -\\\hline
  Closest Depth Pooling & 2.77 & 0.94 & - & - & - & -\\
  Nadaraya Watson\cite{Nadaraya,Watson} & 2.99 & 0.74 & - & - & - & -\\
  ConvNet & 2.97 & 0.78 & - & - & - & -\\
  ConvNet + mask & 2.24 & 0.79 & - & - & - & -\\
  SparseConvNet\cite{uhrig} & 1.82 & 0.58 & 0.035 & 0.33 & 0.65 & 0.82\\
  Ma \& Karaman\cite{sparsetodense}& 1.68 & 0.70 & 0.039 & 0.21 & 0.41 & 0.59\\\hline
  Ours 1 Layer & 2.77 & 0.83 & 0.054 & 0.3 & 0.47 & 0.59\\
  Ours 2 Layers & 1.45 & 0.47 & 0.028 & 0.41 & 0.68 & 0.8\\
  Ours 3 Layers & \textbf{1.35} & \textbf{0.43} & \textbf{0.024} & \textbf{0.48} & \textbf{0.73} & \textbf{0.83}\\
  \hline
 \end{tabular}
 \caption{Validation error of various methods on the KITTI Depth Completion benchmark. All results except for SparseConvNet and Ma's are taken as reported from ~\cite{uhrig}. Our method outperforms all previous state-of-the-art depth only completion methods (Middle) as well as those that use RGB images for guidance (Top).}
  \label{tab:kitti}
\end{table}



% \end{figure}
We evaluate our method on the new KITTI Depth Completion Benchmark~\cite{uhrig} instead of directly comparing against the LiDAR measurements from the raw KITTI dataset. The raw LiDAR points given in KITTI are corrupted by noise, motion of the vehicle during sampling, image rectification artifacts, and accounts to only 4\% of the total number of pixels in the image. Thus it's not ideal for evaluating depth completion systems. Instead, the benchmark proposed in~\cite{uhrig} resolved these issues by accumulating LiDAR measurements from nearby frames in the video sequences, and automatically removing accumulated LiDAR points that deviate too far from the points reconstructed by semi-global matching. This provides quality ground truth and effectively simulates the main application of interest: recovering dense depth from a single LiDAR sweep.

% The KITTI dataset provides raw LIDAR measurements in conjunction with high resolution RGB images from 22 video sequences of driving in both cities and highways. Combining all video sequences gives 93k frames with semi-dense ground truth depth\cite{Geiger2012CVPR}. However, these measurements are corrupted by noise, motion of the vehicle during sampling, and image rectification artifacts. Additionally the raw LIDAR points are very sparse, accounting for only 4\% of the total number of pixels in the map. For these reasons the raw KITTI dataset is not ideal for evaluating depth completion systems.\\

% The sparsity issue can be resolved by accumulating LIDAR measurements from nearby frames in the video sequences, and by using semi-global matching to reconstruct dense 3D points which can be projected using the given calibration files. Both these processes introduce more errors from occlusion and non-rigid movement across frames. Uhrig \etal resolved these issues by automatically removing accumulated LIDAR points that deviate too far from the SGM points. The result is the KITTI depth completion benchmark, the first and only large scale dataset for depth completion training and evaluation. This benchmark is ideal since it has high quality ground truth and effectively simulates the main application of depth completion: recovering dense depth from a single LiDAR sweep.\\

In Table~\ref{tab:kitti}, we form a close comparison against the very deep Sparse-to-Dense network (Ma \& Karaman~\cite{sparsetodense}) and the Sparsity Invariant CNN (SparseConvNet~\cite{uhrig}) which are the current sate-of-the-art deep learning-based method.

% Sparsity Invariant CNN (Sparse)
% Uhrig \etal then evaluated their proposed Sparsity Invariant CNN (SparseConnvNet)~\ciet{Uhrig2017THREEDV} on this dataset as well as several other current methods for depth completion. Figure (\ref{fig:kitti}) shows the results they present along with our method and the results of the very deep Sparse-to-Dense network proposed by Ma \& Karaman. 

The Sparse-to-Dense network uses a similar deep network architecture as those used for single shot depth prediction -- with Resnet-18 as the encoder and up-projection blocks for the decoder. %Similar architectures have been presented in other works for single shot depth prediction. 
While the Sparse-to-Dense network is able to achieve good RMSE, it falls behind the SparseConvNet on MAE. We believe that this is because the deeper network can better estimate the average depth of a region but is unable to predict fine detail, leading to a higher MAE. By comparison, our method is able to both estimate the correct average depth and reconstruct fine detail due to its ability to directly optimize the prediction with respect to the input. Most notably our method outperforms all of the existing methods by a wide margin, including those that use RGB images and those that use orders of magnitude more parameters than our method.

\subsubsection{Varying Sparsity Levels}
\label{sec:vary-spars-levels}
\begin{figure*}
  \centering
  \begin{minipage}{0.45\textwidth}
    \centering
    \includegraphics[width=\linewidth]{sparsity_plot}
    \captionof{figure}{Results on the KITTI benchmark for varying levels of input sparsity. The keep probability represents the probability that any particular LiDAR sample is retained.}
    \label{fig:sparsity}
  \end{minipage}\hspace{1cm}
  \begin{minipage}{0.45\textwidth}
    \centering
    \includegraphics[width=\linewidth]{trainsize_plot}
    \caption{Results of selected methods for varying training set sizes.}
    \label{fig:trainsize}
  \end{minipage}
\end{figure*}

Uhrig \etal show that their Sparsity Invariant CNNs are very robust to a mismatch between the training and testing levels of sparsity. While we do not see a practical use for disparities as large as those tested in ~\cite{uhrig}, we do believe that depth completion systems should perform well under reasonable sparsity changes. To this end we adjusted the level of input sparsity in the KITTI benchmark by dropping input samples with probability $p$, for various values of $p$. The results of this experiment are shown in Figure (\ref{fig:sparsity}). While it is clear that our method does not achieve the level of sparsity invariance of the SparseConvNet, it still outperforms both baseline results even when the only 50\% of the input samples are kept.
\subsection{Effect of Amount of Training Data}
\label{sec:effect-training-data}

Modern deep learning models typically have tens of thousands to millions of parameters and therefore require enormous training sets to achieve good performance. This is in fact the motivation for the KITTI depth completion dataset, since previous benchmarks did not have enough data to train deep networks. In this section we investigate the dependence on the amount of training data on the performance of our method in comparison with a standard deep network and the sparsity invariant variety.

Figure (\ref{fig:trainsize}) shows the results of evaluating these models on the 1k manually selected validation depth maps after training on varying subsets of the 86k training maps. Our method outperforms both baselines for all training sizes. As expected Ma \& Karaman's method fails to generalize well when trained on a small dataset since the model has ~3.4M parameters but performs well once trained on the full dataset. It is interesting to observe that the method of Uhrig \etal does not gain any performance from training on more data. As a result it is ultimately out performed by the deep network which does not take sparsity into account. Our method is able to perform comparably to the sparsity invariant network with only 100 training examples but does increase in performance when given more data, validating the need for learning layered sparse coding dictionaries from large training sets. 
\subsection{Effect of Iterative Optimization}
\label{sec:effect-iter-optim}
\begin{figure}
  \centering
  \includegraphics[width=0.7\textwidth]{iter_plot}
  \caption{Results on the depth completion benchmark for different numbers of ADMM iterations. The total error is shown in blue while the red line shows the error on just those points given as input. The dotted lines show the same metrics but for the SparseConvNet of Uhrig \etal.}
  \label{fig:iterplot}
\end{figure}

In this section we demonstrate that the success of our approach comes from its ability to refine depth estimates over multiple iterations. Applying a feed forward neural network to this problem frames it as finding a mapping from sparse LiDAR points to true depth maps. This is a reasonable approach but it doesn't utilize all of the available information, specifically it doesn't encode the relationship that input samples are a subset of the output that has been corrupted by noise. In contrast, our approach of phrasing depth completion as a compressed sensing missing data problem directly expresses that relationship. By solving this problem in an iterative fashion our network that is able to find depth maps that are both consistent with the input constraints and have sparse representations.\\

The importance of iterative optimization is shown in Figure (\ref{fig:iterplot}) where we examine the performance of our method as a function of the number of ADMM iterations it uses. It is clear that with few iterations our network fails to enforce the constraints and performs comparably to the SparseConvNet.
This is also consistent with Murdock \etal's observation that a feed forward network resembles a single iteration of an ADNN. As we increase the number of iterations our method is able to better optimize its prediction and gains a substantial performance boost.
% - Feed-forward is bad for this problem
% - Phrasing it as optimization is better, specifically sparse coding
% - All the best algorithms for CSC require multiple iterations to converge to a good solution
% - We have evidence for this, more iteration decreases both the total error and the error on the constraint points

% - instead of viewing problem as mapping from sparse input to dense depth we define it as an optimization problem over the given depth values and their sparse codes.
% - 



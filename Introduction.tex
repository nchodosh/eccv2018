\section{Introduction}
    In recent years 3D information has become an important component of robotic sensing. Usually this information in 2.5D as a depth map, either measured directly using LIDAR or computed using stereo correspondence. Since LIDAR and stereo techniques yield few samples relative to modern image sensors, it has become desirable to convert sparse depth measurements into high resolution depth maps.\\
  The field of compressed sensing (CS) provides a natural framework for this  problem of recovering a signal from only a few measurements. Formally, compressed sensing is concerned with solving the following optimization problem:
  \begin{equation}
    \label{eq:1}
    \min \left||x\right||_0 s.t. Ax = b
  \end{equation}
  The key insight is that when the optimal $x$ is known to have only a few non-zero entries, then it is a unique minimum and can be recovered exactly even when $A$ is over-complete and only a few elements of $b$ are known.\\
  The choice of dictionary, $A$, is extremely important for extracting sparse codes $x$ but learning an appropriate $A$ from data proves challenging, especially with a large amount of data or large dimensionality. Learning dictionaries is especially difficult in layered sparse coding where one seeks a very high level sparse representation $x_{\ell}$ such that $b = A_0A_1\ldots A_{\ell}x_{\ell}$ and each intermediate product $x_i = A_iA_{i+1}\ldots A_{\ell}x_{\ell}$ is also sparse. This formulation makes using large effective dictionary computationally tractable, and when the dictionaries have a convolutional structure it allows for increased receptive fields while keeping the number of parameters manageable. For the case of depth map prediction, dictionary learning is further complicated by the fact that we do not have any complete signals to learn from. Dictionary learning algorithms exist for single-layer sparse training data case, and for the multi-level case, but not for learning multi-level codes from sparse data. This is reflected by the fact that recent work applying CS to depth completion has used single-level, hand crafted dictionaries. In this work we propose an algorithm for learning all dictionaries and codes simultaneously from a large dataset of sparse ground-truth.\\
  Unlike CS, deep learning techniques can be directly applied to sparse inputs, sparse ground truth, and large training sets. However, for deep networks to be effective at dealing with sparse inputs they require orders of magnitude more parameters and data than our method while still not performing as well. Urhig et al pointed out the issues of CNNs with sparse inputs and proposed Sparsity Invariant CNNs which achieve good performance on sparse data with much shallower networks. While these new networks explicitly handle the sparsity of the input, they do not explicitly handle the constraint that the input depth map is a subset of the desired output. The recent work by Murdock et al shows a connection between deep learning and sparse coding, where feed forward neural networks are viewed as a single iteration of a sparse coding algorithm. With this perspective they introduce Alternating Direction Neural Networks which internally perform the ADMM optimization algorithm. In this work we propose an extension of ADNNs which explicitly encode the previously mentioned sparsity and input/output constraints.\\
  To summarize the three main contributions of this paper are:
  \begin{enumerate}
    \item Our method provides a way to learn all dictionaries directly from the data. CS dictionary learning algorithms exist, but are limited to single level codes and are not applicable to learning from incomplete training data, as is our case. Additionally, recent applications of CS to depth completion have not used any learning and instead rely on hand crafted dictionaries such as wavelets and contourlets.
  \item Our method uses very few parameters compared to deep learning approaches. We achieve state of the art performance using only two layers of convolutional dictionaries which require several orders of magnitude fewer parameters than modern deep networks. With fewer parameters, our system trains faster and with less data.
  \item Our method allows for explicit encoding of the constraints from the input sparse depth. Current deep learning approaches simply feed a sparse depth map into a network and hope that it learns to identify which inputs represent missing data. Some recent models explicitly include masks which drastically improves CNN performance, but none have a way of encoding that the input is a subset of the desired output. In contrast our method directly optimizes the predicted map with respect to the input.
  \end{enumerate}
  